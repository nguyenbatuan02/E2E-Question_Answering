{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4576536,"sourceType":"datasetVersion","datasetId":2604359},{"sourceId":110683963,"sourceType":"kernelVersion"},{"sourceId":110686471,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame()\n\nall_text = []\nall_id = []\nall_question = []\nall_title = []\nall_category = []\nall_is_long = []\n\nshort_candidate_start = []\nshort_candidate = []\nanswer = []\n\nimport json\n\nwith open('../input/zalo2022-e2e-qa/e2eqa-trainpublic_test-v1/e2eqa-train+public_test-v1/zac2022_train_merged_final.json', 'r') as f:\n    data = json.load(f)\n\n\nfor item in data['data']:\n    all_id.append(item['id'])\n    all_question.append(item['question'])\n    all_text.append(item['text'])\n    all_title.append(item['title'])\n    all_category.append(item['category'])\n    all_is_long.append(item['is_long_answer'])\n    \n    if 'short_candidate_start' in item.keys():\n        \n        short_candidate_start.append(item['short_candidate_start'])\n        \n    else:\n        short_candidate_start.append('')\n    \n    if 'short_candidate' in item.keys():\n        \n        short_candidate.append(item['short_candidate'])\n    else:\n         short_candidate.append('')\n            \n    if 'answer' in item.keys():\n        \n        answer.append(item['answer'])\n    else:\n        answer.append('')\n\n\n    \ndf['id'] = all_id\ndf['text'] = all_text\ndf['title'] = all_title\ndf['question'] = all_question\ndf['category'] = all_category\ndf['is_long_answer'] = all_is_long\ndf['short_candidate_start'] = short_candidate_start\ndf['answer'] = answer\ndf['short_candidate'] = short_candidate\n\n\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-26T14:04:00.340745Z","iopub.execute_input":"2024-01-26T14:04:00.341101Z","iopub.status.idle":"2024-01-26T14:04:00.663717Z","shell.execute_reply.started":"2024-01-26T14:04:00.341071Z","shell.execute_reply":"2024-01-26T14:04:00.662002Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                 id  \\\n0  718d41cd997b2b44b0685ac54aa55bd8   \n1  c926e7b0717202618a10dd907d4b4c39   \n2  d38ef5bf1fb82b410026ed82c8a44cae   \n3  b6b5589a98fdccd208dc752bac853993   \n4  82396a18fa9812bfec4d3ecb7ae60905   \n\n                                                text                title  \\\n0  Thủ tướng Trung Quốc là nhân vật lãnh đạo chín...           Trung Quốc   \n1  có 23 quốc gia không có lực lượng quân đội, ba...                        \n2  Raymondienne (hay Raymonde Dien) sinh ngày 13 ...         Raymondienne   \n3  Cúp cờ vua thế giới là tên gọi một số giải đấu...  Cúp cờ vua thế giới   \n4  Đỉnh núi nằm ở phần trung tâm của dãy núi Đại ...              Shkhara   \n\n                                            question            category  \\\n0                         Thủ tướng Trung Quốc là gì  PARTIAL_ANNOTATION   \n1                     Đất nước nào không có quân đội     FULL_ANNOTATION   \n2  Pháp tấn công xâm lược Việt Nam vào ngày tháng...   FALSE_LONG_ANSWER   \n3                     Cờ vua còn có tên gọi nào khác   FALSE_LONG_ANSWER   \n4                           Núi nào cao nhất châu âu     FULL_ANNOTATION   \n\n   is_long_answer short_candidate_start  \\\n0            True                         \n1            True                    53   \n2           False                         \n3           False                         \n4            True                    73   \n\n                                              answer  \\\n0                                                      \n1  wiki/Danh_sách_quốc_gia_không_có_lực_lượng_vũ_...   \n2                                                      \n3                                                      \n4                                        wiki/Elbrus   \n\n                                     short_candidate  \n0                                                     \n1  Costa Rica, Iceland, Panama, Micronesia, Quần ...  \n2                                                     \n3                                                     \n4                                         núi Elbrus  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>title</th>\n      <th>question</th>\n      <th>category</th>\n      <th>is_long_answer</th>\n      <th>short_candidate_start</th>\n      <th>answer</th>\n      <th>short_candidate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>718d41cd997b2b44b0685ac54aa55bd8</td>\n      <td>Thủ tướng Trung Quốc là nhân vật lãnh đạo chín...</td>\n      <td>Trung Quốc</td>\n      <td>Thủ tướng Trung Quốc là gì</td>\n      <td>PARTIAL_ANNOTATION</td>\n      <td>True</td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c926e7b0717202618a10dd907d4b4c39</td>\n      <td>có 23 quốc gia không có lực lượng quân đội, ba...</td>\n      <td></td>\n      <td>Đất nước nào không có quân đội</td>\n      <td>FULL_ANNOTATION</td>\n      <td>True</td>\n      <td>53</td>\n      <td>wiki/Danh_sách_quốc_gia_không_có_lực_lượng_vũ_...</td>\n      <td>Costa Rica, Iceland, Panama, Micronesia, Quần ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d38ef5bf1fb82b410026ed82c8a44cae</td>\n      <td>Raymondienne (hay Raymonde Dien) sinh ngày 13 ...</td>\n      <td>Raymondienne</td>\n      <td>Pháp tấn công xâm lược Việt Nam vào ngày tháng...</td>\n      <td>FALSE_LONG_ANSWER</td>\n      <td>False</td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6b5589a98fdccd208dc752bac853993</td>\n      <td>Cúp cờ vua thế giới là tên gọi một số giải đấu...</td>\n      <td>Cúp cờ vua thế giới</td>\n      <td>Cờ vua còn có tên gọi nào khác</td>\n      <td>FALSE_LONG_ANSWER</td>\n      <td>False</td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>82396a18fa9812bfec4d3ecb7ae60905</td>\n      <td>Đỉnh núi nằm ở phần trung tâm của dãy núi Đại ...</td>\n      <td>Shkhara</td>\n      <td>Núi nào cao nhất châu âu</td>\n      <td>FULL_ANNOTATION</td>\n      <td>True</td>\n      <td>73</td>\n      <td>wiki/Elbrus</td>\n      <td>núi Elbrus</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:07.381191Z","iopub.execute_input":"2024-01-19T14:38:07.381836Z","iopub.status.idle":"2024-01-19T14:38:07.388637Z","shell.execute_reply.started":"2024-01-19T14:38:07.381798Z","shell.execute_reply":"2024-01-19T14:38:07.387635Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"(20857, 9)"},"metadata":{}}]},{"cell_type":"code","source":"df['category'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:04:06.815399Z","iopub.execute_input":"2024-01-26T14:04:06.815803Z","iopub.status.idle":"2024-01-26T14:04:06.834049Z","shell.execute_reply.started":"2024-01-26T14:04:06.815771Z","shell.execute_reply":"2024-01-26T14:04:06.832933Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"category\nFALSE_LONG_ANSWER     12370\nFULL_ANNOTATION        4989\nPARTIAL_ANNOTATION     3498\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df_full_anno = df[df['category'] == 'FULL_ANNOTATION']\n\ndf_full_anno","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:07.418415Z","iopub.execute_input":"2024-01-19T14:38:07.418837Z","iopub.status.idle":"2024-01-19T14:38:07.459464Z","shell.execute_reply.started":"2024-01-19T14:38:07.418807Z","shell.execute_reply":"2024-01-19T14:38:07.458227Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                     id  \\\n1      c926e7b0717202618a10dd907d4b4c39   \n4      82396a18fa9812bfec4d3ecb7ae60905   \n9      ee8a272bcefac7b96037a1571984b1f8   \n16     c51604b25a641b2665c4ce1784b8cc6a   \n23     04bf774a424133d27c34895960937b87   \n...                                 ...   \n20826  be7f71e3927632f3f3f2afa68ff1b91b   \n20836  5769cee0487a9674306d271854244b39   \n20840  e47529f7ade3caaaa60096137798c6ff   \n20842  935572f75515f8d8ab831648589899c6   \n20852  508022f540c39fe31511f594748759bc   \n\n                                                    text  \\\n1      có 23 quốc gia không có lực lượng quân đội, ba...   \n4      Đỉnh núi nằm ở phần trung tâm của dãy núi Đại ...   \n9      Lịch sử . Ai Cập bị La Mã chiếm năm 30 TCN , v...   \n16     Các nghiên cứu lịch sử cho thấy Cổ Am rất có t...   \n23     Phục Hưng (tiếng Pháp: \"Renaissance\", , , từ \"...   \n...                                                  ...   \n20826  Tiền giấy xuất hiện . Năm 1396 thời Trần Thuận...   \n20836  Máy quay phim: Phát minh kỳ diệu của anh em Lu...   \n20840  Ngũ Hành Sơn hay núi Non Nước là một danh thắn...   \n20842  Trận Như Nguyệt là một trận đánh lớn diễn ra ở...   \n20852  Trong thần thoại Hy Lạp , \" Eros \" là vị thần ...   \n\n                            title  \\\n1                                   \n4                         Shkhara   \n9                      Alexandria   \n16                          Cổ Am   \n23                      Phục Hưng   \n...                           ...   \n20826  Tiền tệ Đại Việt thời Trần   \n20836                               \n20840                Ngũ Hành Sơn   \n20842             Trận Như Nguyệt   \n20852                        Eros   \n\n                                                question         category  \\\n1                         Đất nước nào không có quân đội  FULL_ANNOTATION   \n4                               Núi nào cao nhất châu âu  FULL_ANNOTATION   \n9      Thành phố nào là thủ phủ của Ai Cập trong đế q...  FULL_ANNOTATION   \n16     Ai là người đứng đầu trong cuộc chống lại chín...  FULL_ANNOTATION   \n23           Nền văn hoá Phục Hưng bắt nguồn từ nước nào  FULL_ANNOTATION   \n...                                                  ...              ...   \n20826  vua nào cho phát hành tiền giấy đầu tiên ở việ...  FULL_ANNOTATION   \n20836             Ai là người phát minh ra máy quay phim  FULL_ANNOTATION   \n20840  tên quận nào ở đà nẵng cũng là tên một danh th...  FULL_ANNOTATION   \n20842  trận đánh nào có ý nghĩa quyết định trong khán...  FULL_ANNOTATION   \n20852  trong thần thoại hy lạp vị thần tình yêu có tê...  FULL_ANNOTATION   \n\n       is_long_answer short_candidate_start  \\\n1                True                    53   \n4                True                    73   \n9                True                    48   \n16               True                    65   \n23               True                   205   \n...               ...                   ...   \n20826            True                    54   \n20836            True                    37   \n20840            True                   285   \n20842            True                     0   \n20852            True                    28   \n\n                                                  answer  \\\n1      wiki/Danh_sách_quốc_gia_không_có_lực_lượng_vũ_...   \n4                                            wiki/Elbrus   \n9                                        wiki/Alexandria   \n16                                          wiki/Lê_Chân   \n23                                                wiki/Ý   \n...                                                  ...   \n20826                                     wiki/Hồ_Quý_Ly   \n20836                      wiki/Auguste_và_Louis_Lumière   \n20840                           wiki/Ngũ_Hành_Sơn_(quận)   \n20842                               wiki/Trận_Như_Nguyệt   \n20852                                          wiki/Eros   \n\n                                         short_candidate  \n1      Costa Rica, Iceland, Panama, Micronesia, Quần ...  \n4                                             núi Elbrus  \n9                                             Alexandria  \n16                                               Lê Chân  \n23                                                     Ý  \n...                                                  ...  \n20826                                          Hồ Quý Ly  \n20836                         anh em Lumière Phương Hiền  \n20840                                  quận Ngũ Hành Sơn  \n20842                                    Trận Như Nguyệt  \n20852                                               Eros  \n\n[4989 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>title</th>\n      <th>question</th>\n      <th>category</th>\n      <th>is_long_answer</th>\n      <th>short_candidate_start</th>\n      <th>answer</th>\n      <th>short_candidate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>c926e7b0717202618a10dd907d4b4c39</td>\n      <td>có 23 quốc gia không có lực lượng quân đội, ba...</td>\n      <td></td>\n      <td>Đất nước nào không có quân đội</td>\n      <td>FULL_ANNOTATION</td>\n      <td>True</td>\n      <td>53</td>\n      <td>wiki/Danh_sách_quốc_gia_không_có_lực_lượng_vũ_...</td>\n      <td>Costa Rica, Iceland, Panama, Micronesia, Quần ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>82396a18fa9812bfec4d3ecb7ae60905</td>\n      <td>Đỉnh núi nằm ở phần trung tâm của dãy núi Đại ...</td>\n      <td>Shkhara</td>\n      <td>Núi nào cao nhất châu âu</td>\n      <td>FULL_ANNOTATION</td>\n      <td>True</td>\n      <td>73</td>\n      <td>wiki/Elbrus</td>\n      <td>núi Elbrus</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ee8a272bcefac7b96037a1571984b1f8</td>\n      <td>Lịch sử . Ai Cập bị La Mã chiếm năm 30 TCN , v...</td>\n      <td>Alexandria</td>\n      <td>Thành phố nào là thủ phủ của Ai Cập trong đế q...</td>\n      <td>FULL_ANNOTATION</td>\n      <td>True</td>\n      <td>48</td>\n      <td>wiki/Alexandria</td>\n      <td>Alexandria</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>c51604b25a641b2665c4ce1784b8cc6a</td>\n      <td>Các nghiên cứu lịch sử cho thấy Cổ Am rất có t...</td>\n      <td>Cổ Am</td>\n      <td>Ai là người đứng đầu trong cuộc chống lại chín...</td>\n      <td>FULL_ANNOTATION</td>\n      <td>True</td>\n      <td>65</td>\n      <td>wiki/Lê_Chân</td>\n      <td>Lê Chân</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>04bf774a424133d27c34895960937b87</td>\n      <td>Phục Hưng (tiếng Pháp: \"Renaissance\", , , từ \"...</td>\n      <td>Phục Hưng</td>\n      <td>Nền văn hoá Phục Hưng bắt nguồn từ nước nào</td>\n      <td>FULL_ANNOTATION</td>\n      <td>True</td>\n      <td>205</td>\n      <td>wiki/Ý</td>\n      <td>Ý</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20826</th>\n      <td>be7f71e3927632f3f3f2afa68ff1b91b</td>\n      <td>Tiền giấy xuất hiện . Năm 1396 thời Trần Thuận...</td>\n      <td>Tiền tệ Đại Việt thời Trần</td>\n      <td>vua nào cho phát hành tiền giấy đầu tiên ở việ...</td>\n      <td>FULL_ANNOTATION</td>\n      <td>True</td>\n      <td>54</td>\n      <td>wiki/Hồ_Quý_Ly</td>\n      <td>Hồ Quý Ly</td>\n    </tr>\n    <tr>\n      <th>20836</th>\n      <td>5769cee0487a9674306d271854244b39</td>\n      <td>Máy quay phim: Phát minh kỳ diệu của anh em Lu...</td>\n      <td></td>\n      <td>Ai là người phát minh ra máy quay phim</td>\n      <td>FULL_ANNOTATION</td>\n      <td>True</td>\n      <td>37</td>\n      <td>wiki/Auguste_và_Louis_Lumière</td>\n      <td>anh em Lumière Phương Hiền</td>\n    </tr>\n    <tr>\n      <th>20840</th>\n      <td>e47529f7ade3caaaa60096137798c6ff</td>\n      <td>Ngũ Hành Sơn hay núi Non Nước là một danh thắn...</td>\n      <td>Ngũ Hành Sơn</td>\n      <td>tên quận nào ở đà nẵng cũng là tên một danh th...</td>\n      <td>FULL_ANNOTATION</td>\n      <td>True</td>\n      <td>285</td>\n      <td>wiki/Ngũ_Hành_Sơn_(quận)</td>\n      <td>quận Ngũ Hành Sơn</td>\n    </tr>\n    <tr>\n      <th>20842</th>\n      <td>935572f75515f8d8ab831648589899c6</td>\n      <td>Trận Như Nguyệt là một trận đánh lớn diễn ra ở...</td>\n      <td>Trận Như Nguyệt</td>\n      <td>trận đánh nào có ý nghĩa quyết định trong khán...</td>\n      <td>FULL_ANNOTATION</td>\n      <td>True</td>\n      <td>0</td>\n      <td>wiki/Trận_Như_Nguyệt</td>\n      <td>Trận Như Nguyệt</td>\n    </tr>\n    <tr>\n      <th>20852</th>\n      <td>508022f540c39fe31511f594748759bc</td>\n      <td>Trong thần thoại Hy Lạp , \" Eros \" là vị thần ...</td>\n      <td>Eros</td>\n      <td>trong thần thoại hy lạp vị thần tình yêu có tê...</td>\n      <td>FULL_ANNOTATION</td>\n      <td>True</td>\n      <td>28</td>\n      <td>wiki/Eros</td>\n      <td>Eros</td>\n    </tr>\n  </tbody>\n</table>\n<p>4989 rows × 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data['data'][0]","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:07.461218Z","iopub.execute_input":"2024-01-19T14:38:07.461672Z","iopub.status.idle":"2024-01-19T14:38:07.470324Z","shell.execute_reply.started":"2024-01-19T14:38:07.461621Z","shell.execute_reply":"2024-01-19T14:38:07.468947Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'id': '718d41cd997b2b44b0685ac54aa55bd8',\n 'question': 'Thủ tướng Trung Quốc là gì',\n 'title': 'Trung Quốc',\n 'text': 'Thủ tướng Trung Quốc là nhân vật lãnh đạo chính phủ , chủ trì Quốc vụ viện gồm bốn phó thủ tướng cùng người đứng đầu các bộ và uỷ ban cấp bộ . Chủ tịch nước đương nhiệm là Tập Cận Bình , ông cũng là Tổng Bí thư của Đảng Cộng sản Trung Quốc và Chủ tịch Quân uỷ Trung Quốc , do vậy ông là lãnh đạo tối cao của Trung Quốc .',\n 'category': 'PARTIAL_ANNOTATION',\n 'is_long_answer': True}"},"metadata":{}}]},{"cell_type":"code","source":"data['data'][1]","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:07.471968Z","iopub.execute_input":"2024-01-19T14:38:07.472649Z","iopub.status.idle":"2024-01-19T14:38:07.482231Z","shell.execute_reply.started":"2024-01-19T14:38:07.472606Z","shell.execute_reply":"2024-01-19T14:38:07.481046Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'id': 'c926e7b0717202618a10dd907d4b4c39',\n 'question': 'Đất nước nào không có quân đội',\n 'title': '',\n 'text': 'có 23 quốc gia không có lực lượng quân đội, bao gồm: Costa Rica, Iceland, Panama, Micronesia, Quần đảo Marshall, và Vatican...',\n 'short_candidate_start': 53,\n 'short_candidate': 'Costa Rica, Iceland, Panama, Micronesia, Quần đảo Marshall, và Vatican...',\n 'answer': 'wiki/Danh_sách_quốc_gia_không_có_lực_lượng_vũ_trang',\n 'category': 'FULL_ANNOTATION',\n 'is_long_answer': True}"},"metadata":{}}]},{"cell_type":"code","source":"data['data'][2]","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:07.485814Z","iopub.execute_input":"2024-01-19T14:38:07.486188Z","iopub.status.idle":"2024-01-19T14:38:07.495551Z","shell.execute_reply.started":"2024-01-19T14:38:07.486161Z","shell.execute_reply":"2024-01-19T14:38:07.494404Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'id': 'd38ef5bf1fb82b410026ed82c8a44cae',\n 'question': 'Pháp tấn công xâm lược Việt Nam vào ngày tháng năm nào',\n 'title': 'Raymondienne',\n 'text': 'Raymondienne (hay Raymonde Dien) sinh ngày 13 tháng 5 năm 1929 tại Pháp là một người phụ nữ (đảng viên Đảng Cộng sản Pháp) đã tham gia phong trào đấu tranh chống Pháp tái xâm lược Việt Nam.',\n 'category': 'FALSE_LONG_ANSWER',\n 'is_long_answer': False}"},"metadata":{}}]},{"cell_type":"code","source":"data['data'][1].keys()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:07.497447Z","iopub.execute_input":"2024-01-19T14:38:07.498193Z","iopub.status.idle":"2024-01-19T14:38:07.506525Z","shell.execute_reply.started":"2024-01-19T14:38:07.498150Z","shell.execute_reply":"2024-01-19T14:38:07.505377Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"dict_keys(['id', 'question', 'title', 'text', 'short_candidate_start', 'short_candidate', 'answer', 'category', 'is_long_answer'])"},"metadata":{}}]},{"cell_type":"code","source":"'answer' in data['data'][1].keys()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:07.508199Z","iopub.execute_input":"2024-01-19T14:38:07.508593Z","iopub.status.idle":"2024-01-19T14:38:07.517993Z","shell.execute_reply.started":"2024-01-19T14:38:07.508538Z","shell.execute_reply":"2024-01-19T14:38:07.516836Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import json\nfrom glob import glob\nfrom tqdm import tqdm\nimport re\nfrom nltk import word_tokenize as lib_tokenizer\n\ndict_map = dict({})\n\ndef word_tokenize(text):\n    global dict_map\n    words = text.split()\n    words_norm = []\n    for w in words:\n        if dict_map.get(w, None) is None:\n            dict_map[w] = ' '.join(lib_tokenizer(w)).replace('``', '\"').replace(\"''\", '\"')\n        words_norm.append(dict_map[w])\n    return words_norm\n\ndef strip_answer_string(text):\n    text = text.strip()\n    while text[-1] in '.,/><;:\\'\"[]{}+=-_)(*&^!~`':\n        if text[0] != '(' and text[-1] == ')' and '(' in text:\n            break\n        if text[-1] == '\"' and text[0] != '\"' and text.count('\"') > 1:\n            break\n        text = text[:-1].strip()\n    while text[0] in '.,/><;:\\'\"[]{}+=-_)(*&^!~`':\n        if text[0] == '\"' and text[-1] != '\"' and text.count('\"') > 1:\n            break\n        text = text[1:].strip()\n    text = text.strip()\n    return text\n\n\ndef strip_context(text):\n    text = text.replace('\\n', ' ')\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:04:24.771940Z","iopub.execute_input":"2024-01-26T14:04:24.772357Z","iopub.status.idle":"2024-01-26T14:04:25.303736Z","shell.execute_reply.started":"2024-01-26T14:04:24.772327Z","shell.execute_reply":"2024-01-26T14:04:25.302880Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    print(data['data'][i])","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:04:27.302303Z","iopub.execute_input":"2024-01-26T14:04:27.303443Z","iopub.status.idle":"2024-01-26T14:04:27.308950Z","shell.execute_reply.started":"2024-01-26T14:04:27.303406Z","shell.execute_reply":"2024-01-26T14:04:27.307704Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{'id': '718d41cd997b2b44b0685ac54aa55bd8', 'question': 'Thủ tướng Trung Quốc là gì', 'title': 'Trung Quốc', 'text': 'Thủ tướng Trung Quốc là nhân vật lãnh đạo chính phủ , chủ trì Quốc vụ viện gồm bốn phó thủ tướng cùng người đứng đầu các bộ và uỷ ban cấp bộ . Chủ tịch nước đương nhiệm là Tập Cận Bình , ông cũng là Tổng Bí thư của Đảng Cộng sản Trung Quốc và Chủ tịch Quân uỷ Trung Quốc , do vậy ông là lãnh đạo tối cao của Trung Quốc .', 'category': 'PARTIAL_ANNOTATION', 'is_long_answer': True}\n{'id': 'c926e7b0717202618a10dd907d4b4c39', 'question': 'Đất nước nào không có quân đội', 'title': '', 'text': 'có 23 quốc gia không có lực lượng quân đội, bao gồm: Costa Rica, Iceland, Panama, Micronesia, Quần đảo Marshall, và Vatican...', 'short_candidate_start': 53, 'short_candidate': 'Costa Rica, Iceland, Panama, Micronesia, Quần đảo Marshall, và Vatican...', 'answer': 'wiki/Danh_sách_quốc_gia_không_có_lực_lượng_vũ_trang', 'category': 'FULL_ANNOTATION', 'is_long_answer': True}\n{'id': 'd38ef5bf1fb82b410026ed82c8a44cae', 'question': 'Pháp tấn công xâm lược Việt Nam vào ngày tháng năm nào', 'title': 'Raymondienne', 'text': 'Raymondienne (hay Raymonde Dien) sinh ngày 13 tháng 5 năm 1929 tại Pháp là một người phụ nữ (đảng viên Đảng Cộng sản Pháp) đã tham gia phong trào đấu tranh chống Pháp tái xâm lược Việt Nam.', 'category': 'FALSE_LONG_ANSWER', 'is_long_answer': False}\n","output_type":"stream"}]},{"cell_type":"code","source":"data['data'][4]","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:09.336993Z","iopub.execute_input":"2024-01-19T14:38:09.337528Z","iopub.status.idle":"2024-01-19T14:38:09.350122Z","shell.execute_reply.started":"2024-01-19T14:38:09.337489Z","shell.execute_reply":"2024-01-19T14:38:09.348745Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'id': '82396a18fa9812bfec4d3ecb7ae60905',\n 'question': 'Núi nào cao nhất châu âu',\n 'title': 'Shkhara',\n 'text': 'Đỉnh núi nằm ở phần trung tâm của dãy núi Đại Kavkaz , phía đông nam của núi Elbrus , ngọn núi cao nhất của châu Âu .',\n 'short_candidate_start': 73,\n 'short_candidate': 'núi Elbrus',\n 'answer': 'wiki/Elbrus',\n 'category': 'FULL_ANNOTATION',\n 'is_long_answer': True}"},"metadata":{}}]},{"cell_type":"code","source":"data['data'][4]['text'][73:]","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:09.352151Z","iopub.execute_input":"2024-01-19T14:38:09.352945Z","iopub.status.idle":"2024-01-19T14:38:09.360862Z","shell.execute_reply.started":"2024-01-19T14:38:09.352905Z","shell.execute_reply":"2024-01-19T14:38:09.359787Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'núi Elbrus , ngọn núi cao nhất của châu Âu .'"},"metadata":{}}]},{"cell_type":"code","source":"df_partial_pseudo = pd.read_csv('../input/prepare-data-pseudo-label-partial-anno/df_za2022_partial_anno_pseudo_label.csv')\n\ndef post_process(x):\n    return x.replace(\",\",\"\").replace(\".\",\"\").replace(\"?\",\"\").replace(\"!\",\"\").replace(\")\",\"\").replace(\"(\",\"\")\n\ndf_partial_pseudo[\"pred_answer\"] = df_partial_pseudo[\"pred_answer\"].apply(post_process)\n\ndf_partial_pseudo = df_partial_pseudo[df_partial_pseudo['pred_answer_score'] > 0.9] \n\ndf_partial_pseudo","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:04:31.100774Z","iopub.execute_input":"2024-01-26T14:04:31.101552Z","iopub.status.idle":"2024-01-26T14:04:31.201580Z","shell.execute_reply.started":"2024-01-26T14:04:31.101516Z","shell.execute_reply":"2024-01-26T14:04:31.200573Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                    id  \\\n0     718d41cd997b2b44b0685ac54aa55bd8   \n1     e27bcf72913542ee9de69fac59ad77a9   \n2     b8267523a48de0d700116acdbce2ee95   \n6     155b65ee96bab3f74cafec51616e330f   \n7     55084aee753adde09365012b8498f15a   \n...                                ...   \n3490  d9afa3cf8d6267e8852dc6a846dbfe2e   \n3494  1514053e80d5aadd764f4f5f916df004   \n3495  93c746695c50932ac45ac498a192a3e5   \n3496  278ad127825c085a54fa22116c281f92   \n3497  09ee53a835ea4ed2234aee8161b16d87   \n\n                                               question  \\\n0                            Thủ tướng Trung Quốc là gì   \n1     Tỉnh Đắk Nông được thành lập vào ngày tháng nă...   \n2     Tỉnh nào của nước ta vừa giáp Lào vừa giáp Cam...   \n6                           Hari Won sinh năm bao nhiêu   \n7     Lê Thần Tông là cháu ngoại của nhân vật nổi ti...   \n...                                                 ...   \n3490  Hai thành phố bị thả bom nguyên tử của Nhật Bả...   \n3494       Thời đại Hồng Bàng còn có tên gọi khác là gì   \n3495              Hà Nội có bao nhiêu đơn vị hành chính   \n3496                              Ngày thành lập Google   \n3497                   Ngọn núi nào cao nhất Đông Dương   \n\n                                                 title  \\\n0                                           Trung Quốc   \n1                                             Đắk Nông   \n2                                           Tây Nguyên   \n6                                             Hari Won   \n7                                                  NaN   \n...                                                ...   \n3490  Vụ ném bom nguyên tử xuống Hiroshima và Nagasaki   \n3494                                         Hồng Bàng   \n3495                         Lịch sử hành chính Hà Nội   \n3496                                            Google   \n3497                            Dãy núi Hoàng Liên Sơn   \n\n                                                   text  \\\n0     Thủ tướng Trung Quốc là nhân vật lãnh đạo chín...   \n1     Đắk Nông hay Đắc Nông là một tỉnh ở Tây Nguyên...   \n2     Tây Nguyên là vùng cao nguyên, phía bắc giáp t...   \n6     Hari Won ( 하리원 , \" Hari Nguyễn \" ) tên thật là...   \n7     Trịnh Tùng Lê Thần Tông ra đời ngày 19/11/1607...   \n...                                                 ...   \n3490  Vụ ném bom nguyên tử Hiroshima và Nagasaki là ...   \n3494  Hồng Bàng Thị hay Thời đại Hồng Bàng là một gi...   \n3495  Vào thời điểm hiện tại ( 2017 ) , về mặt hành ...   \n3496  Tên miền www.google.com được đăng ký ngày 15 t...   \n3497  Dãy núi Hoàng Liên Sơn rộng 30 km, chạy dài 18...   \n\n                      pred_answer  pred_answer_start  pred_answer_end  \\\n0     nhân vật lãnh đạo chính phủ                 24               51   \n1         ngày 1 tháng 1 năm 2004                 91              114   \n2                         Kon Tum                289              296   \n6                            1985                110              114   \n7                      Trịnh Tùng                197              208   \n...                           ...                ...              ...   \n3490        Hiroshima và Nagasaki                 21               42   \n3494                Hồng Bàng Thị                  0               13   \n3495                           30                 75               77   \n3496      ngày 7 tháng 9 năm 1998                114              137   \n3497                 Phan Xi Păng                322              334   \n\n      pred_answer_score  \n0              0.991081  \n1              0.999969  \n2              0.999406  \n6              0.971467  \n7              0.999415  \n...                 ...  \n3490           0.999946  \n3494           0.999944  \n3495           0.999760  \n3496           0.999846  \n3497           0.995318  \n\n[1942 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>question</th>\n      <th>title</th>\n      <th>text</th>\n      <th>pred_answer</th>\n      <th>pred_answer_start</th>\n      <th>pred_answer_end</th>\n      <th>pred_answer_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>718d41cd997b2b44b0685ac54aa55bd8</td>\n      <td>Thủ tướng Trung Quốc là gì</td>\n      <td>Trung Quốc</td>\n      <td>Thủ tướng Trung Quốc là nhân vật lãnh đạo chín...</td>\n      <td>nhân vật lãnh đạo chính phủ</td>\n      <td>24</td>\n      <td>51</td>\n      <td>0.991081</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>e27bcf72913542ee9de69fac59ad77a9</td>\n      <td>Tỉnh Đắk Nông được thành lập vào ngày tháng nă...</td>\n      <td>Đắk Nông</td>\n      <td>Đắk Nông hay Đắc Nông là một tỉnh ở Tây Nguyên...</td>\n      <td>ngày 1 tháng 1 năm 2004</td>\n      <td>91</td>\n      <td>114</td>\n      <td>0.999969</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b8267523a48de0d700116acdbce2ee95</td>\n      <td>Tỉnh nào của nước ta vừa giáp Lào vừa giáp Cam...</td>\n      <td>Tây Nguyên</td>\n      <td>Tây Nguyên là vùng cao nguyên, phía bắc giáp t...</td>\n      <td>Kon Tum</td>\n      <td>289</td>\n      <td>296</td>\n      <td>0.999406</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>155b65ee96bab3f74cafec51616e330f</td>\n      <td>Hari Won sinh năm bao nhiêu</td>\n      <td>Hari Won</td>\n      <td>Hari Won ( 하리원 , \" Hari Nguyễn \" ) tên thật là...</td>\n      <td>1985</td>\n      <td>110</td>\n      <td>114</td>\n      <td>0.971467</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>55084aee753adde09365012b8498f15a</td>\n      <td>Lê Thần Tông là cháu ngoại của nhân vật nổi ti...</td>\n      <td>NaN</td>\n      <td>Trịnh Tùng Lê Thần Tông ra đời ngày 19/11/1607...</td>\n      <td>Trịnh Tùng</td>\n      <td>197</td>\n      <td>208</td>\n      <td>0.999415</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3490</th>\n      <td>d9afa3cf8d6267e8852dc6a846dbfe2e</td>\n      <td>Hai thành phố bị thả bom nguyên tử của Nhật Bả...</td>\n      <td>Vụ ném bom nguyên tử xuống Hiroshima và Nagasaki</td>\n      <td>Vụ ném bom nguyên tử Hiroshima và Nagasaki là ...</td>\n      <td>Hiroshima và Nagasaki</td>\n      <td>21</td>\n      <td>42</td>\n      <td>0.999946</td>\n    </tr>\n    <tr>\n      <th>3494</th>\n      <td>1514053e80d5aadd764f4f5f916df004</td>\n      <td>Thời đại Hồng Bàng còn có tên gọi khác là gì</td>\n      <td>Hồng Bàng</td>\n      <td>Hồng Bàng Thị hay Thời đại Hồng Bàng là một gi...</td>\n      <td>Hồng Bàng Thị</td>\n      <td>0</td>\n      <td>13</td>\n      <td>0.999944</td>\n    </tr>\n    <tr>\n      <th>3495</th>\n      <td>93c746695c50932ac45ac498a192a3e5</td>\n      <td>Hà Nội có bao nhiêu đơn vị hành chính</td>\n      <td>Lịch sử hành chính Hà Nội</td>\n      <td>Vào thời điểm hiện tại ( 2017 ) , về mặt hành ...</td>\n      <td>30</td>\n      <td>75</td>\n      <td>77</td>\n      <td>0.999760</td>\n    </tr>\n    <tr>\n      <th>3496</th>\n      <td>278ad127825c085a54fa22116c281f92</td>\n      <td>Ngày thành lập Google</td>\n      <td>Google</td>\n      <td>Tên miền www.google.com được đăng ký ngày 15 t...</td>\n      <td>ngày 7 tháng 9 năm 1998</td>\n      <td>114</td>\n      <td>137</td>\n      <td>0.999846</td>\n    </tr>\n    <tr>\n      <th>3497</th>\n      <td>09ee53a835ea4ed2234aee8161b16d87</td>\n      <td>Ngọn núi nào cao nhất Đông Dương</td>\n      <td>Dãy núi Hoàng Liên Sơn</td>\n      <td>Dãy núi Hoàng Liên Sơn rộng 30 km, chạy dài 18...</td>\n      <td>Phan Xi Păng</td>\n      <td>322</td>\n      <td>334</td>\n      <td>0.995318</td>\n    </tr>\n  </tbody>\n</table>\n<p>1942 rows × 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# df_za2019_pseudo = pd.read_csv('../input/prepare-data-zalo-qa-2019/df_za_2019_pseudo_label.csv')\n\n\n# df_za2019_pseudo[\"pred_answer\"] = df_za2019_pseudo[\"pred_answer\"].apply(post_process)\n\n# df_za2019_pseudo = df_za2019_pseudo[df_za2019_pseudo['pred_answer_score'] > 0.6] \n\n\n# df_za2019_pseudo","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:09.501740Z","iopub.execute_input":"2024-01-19T14:38:09.502147Z","iopub.status.idle":"2024-01-19T14:38:09.507490Z","shell.execute_reply.started":"2024-01-19T14:38:09.502109Z","shell.execute_reply":"2024-01-19T14:38:09.506250Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# df_za2019_pseudo.iloc[0][\"pred_answer\"].replace(\",\",\"\").replace(\".\",\"\").replace(\"?\",\"\").replace(\"!\",\"\").replace(\")\",\"\").replace(\"(\",\"\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:09.509394Z","iopub.execute_input":"2024-01-19T14:38:09.509885Z","iopub.status.idle":"2024-01-19T14:38:09.517956Z","shell.execute_reply.started":"2024-01-19T14:38:09.509846Z","shell.execute_reply":"2024-01-19T14:38:09.516840Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"with open('../input/zalo2022-e2e-qa/zac2022_val.json', 'r') as f:\n    val = json.load(f)\n\nall_valid_id = []\n\nfor item in val['data']:\n    all_valid_id.append(item['id'])","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:04:55.553469Z","iopub.execute_input":"2024-01-26T14:04:55.554223Z","iopub.status.idle":"2024-01-26T14:04:55.578197Z","shell.execute_reply.started":"2024-01-26T14:04:55.554190Z","shell.execute_reply":"2024-01-26T14:04:55.577327Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"len(val['data']), len(all_valid_id)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:04:57.983627Z","iopub.execute_input":"2024-01-26T14:04:57.984768Z","iopub.status.idle":"2024-01-26T14:04:57.994820Z","shell.execute_reply.started":"2024-01-26T14:04:57.984724Z","shell.execute_reply":"2024-01-26T14:04:57.993401Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(749, 749)"},"metadata":{}}]},{"cell_type":"code","source":"item = data['data'][4]\n\ncontext_raw = item['text']\nanswer_raw = item['short_candidate']\nanswer_index_raw = item['short_candidate_start']\n\nif context_raw[answer_index_raw: answer_index_raw + len(answer_raw)] == answer_raw:\n    print('true')\n    context_prev = strip_context(context_raw[:answer_index_raw])\n    answer = strip_answer_string(answer_raw)\n    context_next = strip_context(context_raw[answer_index_raw + len(answer):])\n\n    context_prev = ' '.join(word_tokenize(context_prev))\n    context_next = ' '.join(word_tokenize(context_next))\n    answer = ' '.join(word_tokenize(answer))\n    \n    context = \"{} {} {}\".format(context_prev, answer, context_next).strip()\n    \n    print(context)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:05:01.673991Z","iopub.execute_input":"2024-01-26T14:05:01.674447Z","iopub.status.idle":"2024-01-26T14:05:01.682975Z","shell.execute_reply.started":"2024-01-26T14:05:01.674413Z","shell.execute_reply":"2024-01-26T14:05:01.682067Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"true\nĐỉnh núi nằm ở phần trung tâm của dãy núi Đại Kavkaz , phía đông nam của núi Elbrus , ngọn núi cao nhất của châu Âu .\n","output_type":"stream"}]},{"cell_type":"code","source":"all_train = []\n\nfor item in data['data']:\n    if item['id'] not in all_valid_id:\n        all_train.append(item)\n\ntrain_data = {'data': all_train}","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:05:07.752842Z","iopub.execute_input":"2024-01-26T14:05:07.753265Z","iopub.status.idle":"2024-01-26T14:05:08.286339Z","shell.execute_reply.started":"2024-01-26T14:05:07.753230Z","shell.execute_reply":"2024-01-26T14:05:08.285241Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"len(train_data['data']), len(val['data'])","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:10.237650Z","iopub.execute_input":"2024-01-19T14:38:10.238427Z","iopub.status.idle":"2024-01-19T14:38:10.244917Z","shell.execute_reply.started":"2024-01-19T14:38:10.238399Z","shell.execute_reply":"2024-01-19T14:38:10.243988Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(20107, 749)"},"metadata":{}}]},{"cell_type":"code","source":"def handle_file(data, train = True):\n#     with open(file_path, 'r', encoding='utf-8') as file_read:\n#         # with open('../data-bin/squad/raw/train-v2.0.json', 'r', encoding='utf-8') as file_read:\n#         json_data = json.load(file_read)\n        \n#     qa_data = json_data['data']\n    \n    qa_data = data['data']\n    \n    norm_samples = []\n    \n    ## za 2019 data set pseudo label\n    \n#     if train ==True:\n#         for i, row in df_za2019_pseudo.iterrows():\n\n#             print('zalo2019 pseudo dataset!')\n\n#             context_raw = row['text']\n#             question = row['question']\n\n#             answer_raw = row['pred_answer']\n#             answer_index_raw = row['pred_answer_start']\n\n\n#             if context_raw[answer_index_raw: answer_index_raw + len(answer_raw)] == answer_raw:\n\n#         #                 print('True!!!!!!!!!')\n\n#                 context_prev = strip_context(context_raw[:answer_index_raw])\n#                 answer = strip_answer_string(answer_raw)\n#                 context_next = strip_context(context_raw[answer_index_raw + len(answer):])\n\n#                 context_prev = ' '.join(word_tokenize(context_prev))\n#                 context_next = ' '.join(word_tokenize(context_next))\n#                 answer = ' '.join(word_tokenize(answer))\n#                 question = ' '.join(word_tokenize(question))\n\n#                 context = \"{} {} {}\".format(context_prev, answer, context_next).strip()\n\n#                 norm_samples.append({\n#                     \"context\": context,\n#                     \"question\": question,\n#                     \"answer_text\": answer,\n#                     \"answer_start_idx\": len(\"{} {}\".format(context_prev, answer).strip()) - len(answer)\n#                 })\n\n    #za2022 dataset\n\n    for item in tqdm(qa_data, total=len(qa_data)):\n        \n        #preprocess\n        context_raw = item['text']\n        id_ = item['id']\n        category = item['category']\n        is_long_answer = item['is_long_answer']\n        question = item['question']\n        \n        ## train / valid\n        \n        \n        \n        if category == 'FULL_ANNOTATION':\n            answer_raw = item['short_candidate']\n            answer_index_raw = item['short_candidate_start']\n            \n            if context_raw[answer_index_raw: answer_index_raw + len(answer_raw)] == answer_raw:\n                \n#                 print('True!!!!!!!!!')\n                \n                context_prev = strip_context(context_raw[:answer_index_raw])\n                answer = strip_answer_string(answer_raw)\n                context_next = strip_context(context_raw[answer_index_raw + len(answer):])\n\n                context_prev = ' '.join(word_tokenize(context_prev))\n                context_next = ' '.join(word_tokenize(context_next))\n                answer = ' '.join(word_tokenize(answer))\n                question = ' '.join(word_tokenize(question))\n\n                context = \"{} {} {}\".format(context_prev, answer, context_next).strip()\n\n                norm_samples.append({\n                    \"context\": context,\n                    \"question\": question,\n                    \"answer_text\": answer,\n                    \"answer_start_idx\": len(\"{} {}\".format(context_prev, answer).strip()) - len(answer)\n                })\n                \n        elif category == 'PARTIAL_ANNOTATION':\n            \n            if id_ in df_partial_pseudo['id'].unique():\n                \n              #  print('pseudo data!!')\n                \n                answer_raw = df_partial_pseudo[df_partial_pseudo['id'] == id_]['pred_answer'].values[0]\n                answer_index_raw = df_partial_pseudo[df_partial_pseudo['id'] == id_]['pred_answer_start'].values[0] # item['short_candidate_start']\n\n                if context_raw[answer_index_raw: answer_index_raw + len(answer_raw)] == answer_raw:\n\n    #                 print('True!!!!!!!!!')\n\n                    context_prev = strip_context(context_raw[:answer_index_raw])\n                    answer = strip_answer_string(answer_raw)\n                    context_next = strip_context(context_raw[answer_index_raw + len(answer):])\n\n                    context_prev = ' '.join(word_tokenize(context_prev))\n                    context_next = ' '.join(word_tokenize(context_next))\n                    answer = ' '.join(word_tokenize(answer))\n                    question = ' '.join(word_tokenize(question))\n\n                    context = \"{} {} {}\".format(context_prev, answer, context_next).strip()\n\n                    norm_samples.append({\n                        \"context\": context,\n                        \"question\": question,\n                        \"answer_text\": answer,\n                        \"answer_start_idx\": len(\"{} {}\".format(context_prev, answer).strip()) - len(answer)\n                    })\n                    \n            else:\n                context_raw = ' '.join(word_tokenize(context_raw))\n                question = ' '.join(word_tokenize(question))\n                norm_samples.append({\n                    \"context\": context_raw,\n                    \"question\": question,\n                    \"answer_text\": '',\n                    \"answer_start_idx\": 0\n                })\n                    \n                \n        else:\n            context_raw = ' '.join(word_tokenize(context_raw))\n            question = ' '.join(word_tokenize(question))\n            norm_samples.append({\n                \"context\": context_raw,\n                \"question\": question,\n                \"answer_text\": '',\n                \"answer_start_idx\": 0\n            })\n     \n    \n    print('Len training sample: ', len(norm_samples))\n    return norm_samples","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:05:11.179265Z","iopub.execute_input":"2024-01-26T14:05:11.180069Z","iopub.status.idle":"2024-01-26T14:05:11.202960Z","shell.execute_reply.started":"2024-01-26T14:05:11.180015Z","shell.execute_reply":"2024-01-26T14:05:11.201875Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"dict_data_train = handle_file(train_data, train = True)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:05:15.852832Z","iopub.execute_input":"2024-01-26T14:05:15.853646Z","iopub.status.idle":"2024-01-26T14:05:28.849089Z","shell.execute_reply.started":"2024-01-26T14:05:15.853607Z","shell.execute_reply":"2024-01-26T14:05:28.847954Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"100%|██████████| 20107/20107 [00:12<00:00, 1548.24it/s]","output_type":"stream"},{"name":"stdout","text":"Len training sample:  19867\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('qa_full_anno_train.jsonl', 'w', encoding='utf-8') as file:\n    for item in dict_data_train:\n        file.write(\"{}\\n\".format(json.dumps(item, ensure_ascii=False)))\n\nprint(\"Total: {} samples\".format(len(dict_data_train)))","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:24.028082Z","iopub.execute_input":"2024-01-19T14:38:24.028419Z","iopub.status.idle":"2024-01-19T14:38:24.344745Z","shell.execute_reply.started":"2024-01-19T14:38:24.028392Z","shell.execute_reply":"2024-01-19T14:38:24.343631Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Total: 19867 samples\n","output_type":"stream"}]},{"cell_type":"code","source":"dict_data_valid = handle_file(val, train = False)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:24.346355Z","iopub.execute_input":"2024-01-19T14:38:24.346801Z","iopub.status.idle":"2024-01-19T14:38:24.511252Z","shell.execute_reply.started":"2024-01-19T14:38:24.346764Z","shell.execute_reply":"2024-01-19T14:38:24.510119Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"100%|██████████| 749/749 [00:00<00:00, 4817.88it/s]","output_type":"stream"},{"name":"stdout","text":"Len training sample:  749\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('qa_full_anno_valid.jsonl', 'w', encoding='utf-8') as file:\n    for item in dict_data_valid:\n        file.write(\"{}\\n\".format(json.dumps(item, ensure_ascii=False)))\n\nprint(\"Total: {} samples\".format(len(dict_data_valid)))","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:24.512614Z","iopub.execute_input":"2024-01-19T14:38:24.512949Z","iopub.status.idle":"2024-01-19T14:38:24.532261Z","shell.execute_reply.started":"2024-01-19T14:38:24.512923Z","shell.execute_reply":"2024-01-19T14:38:24.531138Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Total: 749 samples\n","output_type":"stream"}]},{"cell_type":"code","source":"import datasets\nfrom datasets import concatenate_datasets\nimport glob\n\n# datasets.temp_seed(101)\ndatasets.disable_progress_bar()\n\n\ndef assert_sample(sample):\n    assert sample['context'][sample['answer_start_idx']: sample['answer_start_idx'] + len(sample['answer_text'])] == \\\n           sample['answer_text'], sample\n    assert len(sample['context']) > 0\n    assert len(sample['question']) > 0\n    return True\n\n\ndef format_sample(sample):\n    context_prev = sample['context'][:sample['answer_start_idx']].split()\n    sample['answer_word_start_idx'] = len(context_prev)\n    sample['answer_word_end_idx'] = len(context_prev) + len(sample['answer_text'].split()) - 1\n    return sample\n\n\nif __name__ == \"__main__\":\n    train_set = []\n    valid_set = []\n    for part in ['/kaggle/working/qa_full_anno_train.jsonl']:  #glob.glob('../data-bin/unify/*.jsonl'):\n        dataset = datasets.load_dataset('json', data_files=[part])['train']\n        dataset.filter(assert_sample)\n        dataset = dataset.map(format_sample)\n\n#         all_data = dataset.train_test_split(test_size=0.1)\n#         train = all_data['train']\n#         valid = all_data['test']\n#         train_set.append(train)\n#         valid_set.append(valid)\n        train_set.append(dataset)\n        \n    for part in ['/kaggle/working/qa_full_anno_valid.jsonl']:  #glob.glob('../data-bin/unify/*.jsonl'):\n        dataset = datasets.load_dataset('json', data_files=[part])['train']\n        dataset.filter(assert_sample)\n        dataset = dataset.map(format_sample)\n\n#         all_data = dataset.train_test_split(test_size=0.1)\n#         train = all_data['train']\n#         valid = all_data['test']\n#         train_set.append(train)\n#         valid_set.append(valid)\n        valid_set.append(dataset)\n\n    train_dataset = concatenate_datasets(train_set)\n    valid_dataset = concatenate_datasets(valid_set)\n\n    train_dataset.save_to_disk('/kaggle/working/data-bin/processed/train.dataset')\n    valid_dataset.save_to_disk('/kaggle/working/data-bin/processed/valid.dataset')\n\n    print(\"Train: {} samples\".format(len(train_dataset)))\n    print(\"Valid: {} samples\".format(len(valid_dataset)))","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:24.533705Z","iopub.execute_input":"2024-01-19T14:38:24.534041Z","iopub.status.idle":"2024-01-19T14:38:28.575527Z","shell.execute_reply.started":"2024-01-19T14:38:24.534008Z","shell.execute_reply":"2024-01-19T14:38:28.574451Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-0da820d98ead22ab/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0da820d98ead22ab/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-483bab50edee59b4/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-483bab50edee59b4/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\nTrain: 19867 samples\nValid: 749 samples\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:28.581272Z","iopub.execute_input":"2024-01-19T14:38:28.581767Z","iopub.status.idle":"2024-01-19T14:38:28.588523Z","shell.execute_reply.started":"2024-01-19T14:38:28.581737Z","shell.execute_reply":"2024-01-19T14:38:28.587383Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['context', 'question', 'answer_text', 'answer_start_idx', 'answer_word_start_idx', 'answer_word_end_idx'],\n    num_rows: 19867\n})"},"metadata":{}}]},{"cell_type":"code","source":"valid_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:28.589752Z","iopub.execute_input":"2024-01-19T14:38:28.590020Z","iopub.status.idle":"2024-01-19T14:38:28.630988Z","shell.execute_reply.started":"2024-01-19T14:38:28.589996Z","shell.execute_reply":"2024-01-19T14:38:28.630013Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['context', 'question', 'answer_text', 'answer_start_idx', 'answer_word_start_idx', 'answer_word_end_idx'],\n    num_rows: 749\n})"},"metadata":{}}]},{"cell_type":"code","source":"!pip install -U transformers","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:28.632492Z","iopub.execute_input":"2024-01-19T14:38:28.633346Z","iopub.status.idle":"2024-01-19T14:38:43.074130Z","shell.execute_reply.started":"2024-01-19T14:38:28.633319Z","shell.execute_reply":"2024-01-19T14:38:43.072919Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n","output_type":"stream"}]},{"cell_type":"code","source":"#%%writefile train.py \n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nfrom transformers import TrainingArguments\n# from model.mrc_model import MRCQuestionAnswering\nfrom transformers import Trainer\n# from utils import data_loader\nimport numpy as np\nfrom datasets import load_metric\nimport os\n\nimport datasets\nfrom transformers import AutoTokenizer,  AutoModelForCausalLM, AutoConfig\nfrom torch.utils.data import DataLoader\nimport torch\nimport numpy as np\nfrom nltk import word_tokenize\n\n#tokenizer = AutoTokenizer.from_pretrained(\"nguyenvulebinh/vi-mrc-large\")\n\n# tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\",\n# #                                           cache_dir='./model-bin/cache',\n#                                           #local_files_only=True\n#                                          )\n\n\ndef compute_metrics(eval_pred):\n    metric = datasets.load_metric(\"squad\", cache_dir='./log/metric')\n    # print(eval_pred)\n    logits, labels = eval_pred\n    logits = list(zip(logits[0], logits[1]))\n    labels, span_ids, samples_input_ids, word_lengths = list(zip(labels[0], labels[1])), labels[2], labels[3], labels[4]\n    predictions = []\n    references = []\n    for idx, (predict, span_truth, input_ids, sample_words_length) in enumerate(\n            list(zip(logits, span_ids, samples_input_ids, word_lengths))):\n        span_truth = np.delete(span_truth, np.where(span_truth == -100))\n        input_ids = np.delete(input_ids, np.where(input_ids == -100))\n\n        # Get the most likely beginning of answer with the argmax of the score\n        answer_start = sum(sample_words_length[:np.argmax(predict[0])])\n        # Get the most likely end of answer with the argmax of the score\n        answer_end = sum(sample_words_length[:np.argmax(predict[1]) + 1])\n\n        answer = tokenizer.convert_tokens_to_string(\n            tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n        answer_truth = tokenizer.convert_tokens_to_string(\n            tokenizer.convert_ids_to_tokens(span_truth))\n\n        predictions.append({'prediction_text': answer, 'id': str(idx)})\n        references.append({'answers': {'answer_start': [answer_start], 'text': [answer_truth]}, 'id': str(idx)})\n    results = metric.compute(predictions=predictions, references=references)\n    return results\n\n\ndef data_collator(samples):\n    if len(samples) == 0:\n        return {}\n\n    # for sample in samples:\n    #     start_idx = sum(sample['words_lengths'][:sample['start_idx']])\n    #     end_idx = sum(sample['words_lengths'][:sample['end_idx'] + 1])\n    #     if len(sample['answer_text']) > 0:\n    #         answer_predict = set(\n    #             list(' '.join(word_tokenize(tokenizer.decode(sample['input_ids'][start_idx:end_idx])))))\n    #         answer_truth = set(list(' '.join(word_tokenize(sample['answer_text']))))\n    #         if answer_predict != answer_truth:\n    #             print(\"{}\\n{}\\n\\n-------\\n\".format(\n    #                 ' '.join(word_tokenize(tokenizer.decode(sample['input_ids'][start_idx:end_idx]))),\n    #                 ' '.join(word_tokenize(sample['answer_text'])).replace('``', '\"').replace(\"''\", '\"'),\n    #                 sample))\n\n    for sample in samples:\n        start_idx = sum(sample['words_lengths'][:sample['start_idx']])\n        end_idx = sum(sample['words_lengths'][:sample['end_idx'] + 1])\n        sample['span_answer_ids'] = sample['input_ids'][start_idx:end_idx]\n\n    def collate_tokens(values, pad_idx, eos_idx=None, left_pad=False, move_eos_to_beginning=False):\n        \"\"\"Convert a list of 1d tensors into a padded 2d tensor.\"\"\"\n        size = max(v.size(0) for v in values)\n        res = values[0].new(len(values), size).fill_(pad_idx)\n\n        def copy_tensor(src, dst):\n            assert dst.numel() == src.numel()\n            if move_eos_to_beginning:\n                assert src[-1] == eos_idx\n                dst[0] = eos_idx\n                dst[1:] = src[:-1]\n            else:\n                dst.copy_(src)\n\n        for i, v in enumerate(values):\n            copy_tensor(v, res[i][size - len(v):] if left_pad else res[i][:len(v)])\n        return res\n\n    input_ids = collate_tokens([torch.tensor(item['input_ids']) for item in samples], pad_idx=tokenizer.pad_token_id)\n    attention_mask = torch.zeros_like(input_ids)\n    for i in range(len(samples)):\n        attention_mask[i][:len(samples[i]['input_ids'])] = 1\n    words_lengths = collate_tokens([torch.tensor(item['words_lengths']) for item in samples], pad_idx=0)\n    answer_start = collate_tokens([torch.tensor([item['start_idx']]) for item in samples], pad_idx=0)\n    answer_end = collate_tokens([torch.tensor([item['end_idx']]) for item in samples], pad_idx=0)\n    span_answer_ids = collate_tokens([torch.tensor(item['span_answer_ids']) for item in samples],\n                                     pad_idx=-100)\n\n    batch_samples = {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'words_lengths': words_lengths,\n        'start_positions': answer_start,\n        'end_positions': answer_end,\n        'span_answer_ids': span_answer_ids\n    }\n\n    return batch_samples\n\n\ndef tokenize_function(example):\n    example[\"question\"] = example[\"question\"].split()\n    example[\"context\"] = example[\"context\"].split()\n    # max_len_single_sentence = tokenizer.max_len_single_sentence\n    max_len_single_sentence = 368\n\n    question_sub_words_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(w)) for w in example[\"question\"]]\n    context_sub_words_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(w)) for w in example[\"context\"]]\n    valid = True\n    if len([j for i in question_sub_words_ids + context_sub_words_ids for j in\n            i]) > max_len_single_sentence - 1:\n        question_ids = [j for i in question_sub_words_ids for j in i]\n        context_ids = [j for i in context_sub_words_ids[:example['answer_word_end_idx'] + 1] for j in i]\n        remain_tokens = max_len_single_sentence - 1 - len(question_ids)\n        if len(question_ids + context_ids) < max_len_single_sentence - 1:\n            context_sub_words_ids_revise = context_sub_words_ids[:example['answer_word_end_idx'] + 1]\n            idx = example['answer_word_end_idx'] + 1\n            while len([j for i in (context_sub_words_ids_revise + [context_sub_words_ids[idx]]) for j in\n                       i]) < remain_tokens and idx < len(context_sub_words_ids):\n                context_sub_words_ids_revise.append(context_sub_words_ids[idx])\n                idx += 1\n            context_sub_words_ids = context_sub_words_ids_revise\n        else:\n            valid = False\n\n    question_sub_words_ids = [[tokenizer.bos_token_id]] + question_sub_words_ids + [[tokenizer.eos_token_id]]\n    context_sub_words_ids = context_sub_words_ids + [[tokenizer.eos_token_id]]\n\n    input_ids = [j for i in question_sub_words_ids + context_sub_words_ids for j in i]\n    if len(input_ids) > max_len_single_sentence + 2:\n        valid = False\n\n    words_lengths = [len(item) for item in question_sub_words_ids + context_sub_words_ids]\n\n    return {\n        \"input_ids\": input_ids,\n        \"words_lengths\": words_lengths,\n        \"start_idx\": (example['answer_word_start_idx'] + len(question_sub_words_ids)) if len(\n            example[\"answer_text\"]) > 0 else 0,\n        \"end_idx\": (example['answer_word_end_idx'] + len(question_sub_words_ids)) if len(\n            example[\"answer_text\"]) > 0 else 0,\n        \"valid\": valid\n    }\n\n\ndef get_dataloader(train_path, valid_path, batch_size=2, num_proc=10):\n    train_set = datasets.load_from_disk(train_path)\n    valid_set = datasets.load_from_disk(valid_path)\n    print(\"Train set: \", len(train_set))\n    print(\"Valid set: \", len(valid_set))\n    # unique_tags = set(tag for doc in tags for tag in train_set)\n    train_set = train_set.shuffle().map(tokenize_function, batched=False, num_proc=num_proc).filter(\n        lambda example: example['valid'], num_proc=num_proc)\n    valid_set = valid_set.map(tokenize_function, batched=False, num_proc=num_proc).filter(\n        lambda example: example['valid'], num_proc=num_proc)\n    # train_set = train_set.sort('src_ids_len')\n    # valid_set = valid_set.sort('src_ids_len')\n\n    print(\"Train set: \", len(train_set))\n    print(\"Valid set: \", len(valid_set))\n    return train_set, valid_set\n    #return train_set, valid_set.shard(500, 0)\n    #\n    # train_dataloader = DataLoader(\n    #     train_set, shuffle=True, batch_size=batch_size, collate_fn=data_collator\n    # )\n    # valid_dataloader = DataLoader(\n    #     valid_set, batch_size=batch_size, collate_fn=data_collator\n    # )\n    #\n    # return train_dataloader, valid_dataloader\n\n\ndef build_target_dictionary():\n    data_set = datasets.load_from_disk('./data-bin/processed/train.dataset')\n    labels = set([item['language'] for item in data_set])\n    labels2id = {tag: idx for idx, tag in enumerate(labels)}\n    # id2labels = {idx: tag for tag, idx in labels2id.items()}\n\n    tags2id = {\n        \"same\": 1,\n        \"change\": 0\n    }\n    # id2tags = {idx: tag for tag, idx in tags2id.items()}\n\n    return labels2id, tags2id\n\n\n# if __name__ == \"__main__\":\n\n#     train_dataset, valid_dataset = get_dataloader(\n#         train_path='../data-bin/processed/train.dataset',\n#         valid_path='../data-bin/processed/valid.dataset'\n#     )\n#     from tqdm import tqdm\n\n#     for batch in tqdm(train_dataset):\n#         pass\n#         # print(batch['tgt_words_ids'].shape)\n\n#     for batch in tqdm(valid_dataset):\n#         pass\n#         # print(batch['tgt_words_ids'].shape)\n\n# from transformers.models.roberta.modeling_roberta import *\n# _CHECKPOINT_FOR_DOC = \"roberta-base\"\n# _CONFIG_FOR_DOC = \"RobertaConfig\"\n# _TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\n\n\n# class MRCQuestionAnswering(RobertaPreTrainedModel):\n#     config_class = RobertaConfig\n\n# #     def _reorder_cache(self, past, beam_idx):\n# #         pass\n\n# #     _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n# #     _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n#     def __init__(self, config):\n#         super().__init__(config)\n#         self.num_labels = config.num_labels\n\n#         self.roberta = RobertaModel(config, add_pooling_layer=False)\n#         self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n#         self.init_weights()\n\n#     def forward(\n#             self,\n#             input_ids=None,\n#             words_lengths=None,\n#             start_idx=None,\n#             end_idx=None,\n#             attention_mask=None,\n#             token_type_ids=None,\n#             position_ids=None,\n#             head_mask=None,\n#             inputs_embeds=None,\n#             start_positions=None,\n#             end_positions=None,\n#             span_answer_ids=None,\n#             output_attentions=None,\n#             output_hidden_states=None,\n#             return_dict=None,\n#     ):\n#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n#         outputs = self.roberta(\n#             input_ids,\n#             attention_mask=attention_mask,\n#             token_type_ids=token_type_ids,\n#             position_ids=position_ids,\n#             head_mask=head_mask,\n#             inputs_embeds=inputs_embeds,\n#             output_attentions=output_attentions,\n#             output_hidden_states=output_hidden_states,\n#             return_dict=return_dict,\n#         )\n\n#         sequence_output = outputs[0]\n\n#         context_embedding = sequence_output\n\n#         # Compute align word sub_word matrix\n#         batch_size = input_ids.shape[0]\n#         max_sub_word = input_ids.shape[1]\n#         max_word = words_lengths.shape[1]\n#         align_matrix = torch.zeros((batch_size, max_word, max_sub_word))\n\n#         for i, sample_length in enumerate(words_lengths):\n#             for j in range(len(sample_length)):\n#                 start_idx = torch.sum(sample_length[:j])\n#                 align_matrix[i][j][start_idx: start_idx + sample_length[j]] = 1 if sample_length[j] > 0 else 0\n\n#         align_matrix = align_matrix.to(context_embedding.device)\n#         # Combine sub_word features to make word feature\n#         context_embedding_align = torch.bmm(align_matrix, context_embedding)\n\n#         logits = self.qa_outputs(context_embedding_align)\n#         start_logits, end_logits = logits.split(1, dim=-1)\n#         start_logits = start_logits.squeeze(-1).contiguous()\n#         end_logits = end_logits.squeeze(-1).contiguous()\n\n#         total_loss = None\n#         if start_positions is not None and end_positions is not None:\n#             # If we are on multi-GPU, split add a dimension\n#             if len(start_positions.size()) > 1:\n#                 start_positions = start_positions.squeeze(-1)\n#             if len(end_positions.size()) > 1:\n#                 end_positions = end_positions.squeeze(-1)\n#             # sometimes the start/end positions are outside our model inputs, we ignore these terms\n#             ignored_index = start_logits.size(1)\n#             start_positions = start_positions.clamp(0, ignored_index)\n#             end_positions = end_positions.clamp(0, ignored_index)\n\n#             loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n#             start_loss = loss_fct(start_logits, start_positions)\n#             end_loss = loss_fct(end_logits, end_positions)\n#             total_loss = (start_loss + end_loss) / 2\n\n#         if not return_dict:\n#             output = (start_logits, end_logits) + outputs[2:]\n#             return ((total_loss,) + output) if total_loss is not None else output\n\n#         return QuestionAnsweringModelOutput(\n#             loss=total_loss,\n#             start_logits=start_logits,\n#             end_logits=end_logits,\n#             hidden_states=outputs.hidden_states,\n#             attentions=outputs.attentions,\n#         )\n\n#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:43.076212Z","iopub.execute_input":"2024-01-19T14:38:43.076633Z","iopub.status.idle":"2024-01-19T14:38:59.954885Z","shell.execute_reply.started":"2024-01-19T14:38:43.076580Z","shell.execute_reply":"2024-01-19T14:38:59.953758Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# !conda install -y gcc=12.1.0","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:59.958774Z","iopub.execute_input":"2024-01-19T14:38:59.959392Z","iopub.status.idle":"2024-01-19T14:38:59.964065Z","shell.execute_reply.started":"2024-01-19T14:38:59.959360Z","shell.execute_reply":"2024-01-19T14:38:59.963059Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"!pip install transformers\n!pip install peft\n!pip install accelerate -U\n!pip install -q bitsandbytes datasets accelerate loralib\n!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:38:59.965716Z","iopub.execute_input":"2024-01-19T14:38:59.966213Z","iopub.status.idle":"2024-01-19T14:41:09.767593Z","shell.execute_reply.started":"2024-01-19T14:38:59.966165Z","shell.execute_reply":"2024-01-19T14:41:09.765983Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nCollecting peft\n  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/8b/1b/aee2a330d050c493642d59ba6af51f3910cb138ea48ede228c84c204a5af/peft-0.7.1-py3-none-any.whl.metadata\n  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.36.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.1)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.20.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.7.1\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\nCollecting accelerate\n  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/a6/b9/44623bdb05595481107153182e7f4b9f2ef9d3b674938ad13842054dcbd8/accelerate-0.26.1-py3-none-any.whl.metadata\n  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.25.0\n    Uninstalling accelerate-0.25.0:\n      Successfully uninstalled accelerate-0.25.0\nSuccessfully installed accelerate-0.26.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi -L","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:41:09.769512Z","iopub.execute_input":"2024-01-19T14:41:09.769909Z","iopub.status.idle":"2024-01-19T14:41:10.867511Z","shell.execute_reply.started":"2024-01-19T14:41:09.769877Z","shell.execute_reply":"2024-01-19T14:41:10.866344Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-321fd1a1-d91b-6aa3-8b3b-38d698ad2c30)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"bigscience/bloom-3b\", \n    #load_in_8bit=True, \n    #device_map='auto',\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:41:10.869021Z","iopub.execute_input":"2024-01-19T14:41:10.869337Z","iopub.status.idle":"2024-01-19T14:41:21.915517Z","shell.execute_reply.started":"2024-01-19T14:41:10.869308Z","shell.execute_reply":"2024-01-19T14:41:21.914272Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67ddf018b34c45dfb5f8b0b3510c7c4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33af8edf7a9b44d0a6678eb8d7565bab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ce022521df407bbc2477117b2eabb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b44d4df2bfa465d921f87b09ecfe8b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eedcf11a2f94b64b47a0d5c6fadb311"}},"metadata":{}}]},{"cell_type":"code","source":"import peft\nfrom peft import LoraConfig, get_peft_model\nlora_config = LoraConfig(\n    r=1,\n    lora_alpha=1, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n    target_modules=[\"query_key_value\"],\n    lora_dropout=0.05,\n    bias=\"none\", # this specifies if the bias parameter should be trained.\n    task_type=\"CAUSAL_LM\"\n)\npeft_model = get_peft_model(model, lora_config)\nprint(peft_model.print_trainable_parameters())","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:41:21.916844Z","iopub.execute_input":"2024-01-19T14:41:21.917198Z","iopub.status.idle":"2024-01-19T14:41:22.012496Z","shell.execute_reply.started":"2024-01-19T14:41:21.917168Z","shell.execute_reply":"2024-01-19T14:41:22.011136Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"trainable params: 98,304 || all params: 559,312,896 || trainable%: 0.01757585078102687\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset, valid_dataset = get_dataloader(\n    train_path='./data-bin/processed/train.dataset',\n    valid_path='./data-bin/processed/valid.dataset'\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:41:22.013904Z","iopub.execute_input":"2024-01-19T14:41:22.014894Z","iopub.status.idle":"2024-01-19T14:42:08.323676Z","shell.execute_reply.started":"2024-01-19T14:41:22.014863Z","shell.execute_reply":"2024-01-19T14:42:08.322397Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Train set:  19867\nValid set:  749\nTrain set:  19864\nValid set:  749\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Subset\nsubset_size = 200\nsuset_size = 20\nsubset_train_dataset = Subset(train_dataset, range(subset_size))\nsubset_valid_dataset = Subset(valid_dataset, range(suset_size))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:42:08.325735Z","iopub.execute_input":"2024-01-19T14:42:08.326174Z","iopub.status.idle":"2024-01-19T14:42:08.332702Z","shell.execute_reply.started":"2024-01-19T14:42:08.326133Z","shell.execute_reply":"2024-01-19T14:42:08.331653Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:42:08.333958Z","iopub.execute_input":"2024-01-19T14:42:08.334262Z","iopub.status.idle":"2024-01-19T14:42:08.349273Z","shell.execute_reply.started":"2024-01-19T14:42:08.334237Z","shell.execute_reply":"2024-01-19T14:42:08.348359Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['context', 'question', 'answer_text', 'answer_start_idx', 'answer_word_start_idx', 'answer_word_end_idx', 'input_ids', 'words_lengths', 'start_idx', 'end_idx', 'valid'],\n    num_rows: 19864\n})"},"metadata":{}}]},{"cell_type":"code","source":"valid_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:42:08.350600Z","iopub.execute_input":"2024-01-19T14:42:08.350971Z","iopub.status.idle":"2024-01-19T14:42:08.363376Z","shell.execute_reply.started":"2024-01-19T14:42:08.350936Z","shell.execute_reply":"2024-01-19T14:42:08.362319Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['context', 'question', 'answer_text', 'answer_start_idx', 'answer_word_start_idx', 'answer_word_end_idx', 'input_ids', 'words_lengths', 'start_idx', 'end_idx', 'valid'],\n    num_rows: 749\n})"},"metadata":{}}]},{"cell_type":"code","source":"import transformers\nfrom transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n\ntraining_args = TrainingArguments(\n    output_dir='./result',\n    overwrite_output_dir = True,\n    warmup_ratio=0.1,\n    lr_scheduler_type='cosine',\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    #max_steps=50,\n    logging_steps=10,\n    eval_steps=10,\n    evaluation_strategy='steps',\n    report_to=\"tensorboard\"\n)\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=subset_train_dataset,\n    eval_dataset=subset_valid_dataset,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    tokenizer=tokenizer\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:42:08.364514Z","iopub.execute_input":"2024-01-19T14:42:08.364992Z","iopub.status.idle":"2024-01-19T14:42:58.205287Z","shell.execute_reply.started":"2024-01-19T14:42:08.364962Z","shell.execute_reply":"2024-01-19T14:42:58.204262Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [36/36 00:46, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>4.873500</td>\n      <td>4.861454</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>4.782500</td>\n      <td>4.777814</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>4.735200</td>\n      <td>4.730532</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=36, training_loss=4.777361869812012, metrics={'train_runtime': 48.6494, 'train_samples_per_second': 12.333, 'train_steps_per_second': 0.74, 'total_flos': 173144047878144.0, 'train_loss': 4.777361869812012, 'epoch': 2.88})"},"metadata":{}}]},{"cell_type":"code","source":"valid_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:42:58.206919Z","iopub.execute_input":"2024-01-19T14:42:58.207694Z","iopub.status.idle":"2024-01-19T14:42:58.214059Z","shell.execute_reply.started":"2024-01-19T14:42:58.207650Z","shell.execute_reply":"2024-01-19T14:42:58.213072Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['context', 'question', 'answer_text', 'answer_start_idx', 'answer_word_start_idx', 'answer_word_end_idx', 'input_ids', 'words_lengths', 'start_idx', 'end_idx', 'valid'],\n    num_rows: 749\n})"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprediction = trainer.predict(subset_valid_dataset).predictions\nprint(prediction)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:42:58.215463Z","iopub.execute_input":"2024-01-19T14:42:58.215861Z","iopub.status.idle":"2024-01-19T14:43:04.702735Z","shell.execute_reply.started":"2024-01-19T14:42:58.215831Z","shell.execute_reply":"2024-01-19T14:43:04.701584Z"},"trusted":true},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"[[[ 258.0801     266.8064     269.47488   ...  153.17801    153.17807\n    153.17317  ]\n  [ 258.0801     266.8064     269.47488   ...  153.17801    153.17807\n    153.17317  ]\n  [ 258.0801     266.8064     269.47488   ...  153.17801    153.17807\n    153.17317  ]\n  ...\n  [-100.        -100.        -100.        ... -100.        -100.\n   -100.       ]\n  [-100.        -100.        -100.        ... -100.        -100.\n   -100.       ]\n  [-100.        -100.        -100.        ... -100.        -100.\n   -100.       ]]\n\n [[ 298.10687    308.89746    313.13843   ...  177.13327    177.13324\n    177.12787  ]\n  [ 298.10687    308.89746    313.13843   ...  177.13327    177.13324\n    177.12787  ]\n  [ 298.10687    308.89746    313.13843   ...  177.13327    177.13324\n    177.12787  ]\n  ...\n  [-100.        -100.        -100.        ... -100.        -100.\n   -100.       ]\n  [-100.        -100.        -100.        ... -100.        -100.\n   -100.       ]\n  [-100.        -100.        -100.        ... -100.        -100.\n   -100.       ]]\n\n [[ 279.19778    283.52106    279.21146   ...  164.84062    164.84093\n    164.83743  ]\n  [ 279.19778    283.52106    279.21146   ...  164.84062    164.84093\n    164.83743  ]\n  [ 279.19778    283.52106    279.21146   ...  164.84062    164.84093\n    164.83743  ]\n  ...\n  [-100.        -100.        -100.        ... -100.        -100.\n   -100.       ]\n  [-100.        -100.        -100.        ... -100.        -100.\n   -100.       ]\n  [-100.        -100.        -100.        ... -100.        -100.\n   -100.       ]]\n\n ...\n\n [[ 140.90637    148.60526    149.80338   ...   83.46308     83.463234\n     83.45933  ]\n  [ 140.90637    148.60526    149.80338   ...   83.46308     83.463234\n     83.45933  ]\n  [ 140.90637    148.60526    149.80338   ...   83.46308     83.463234\n     83.45933  ]\n  ...\n  [ 310.80606    321.46457    331.7721    ...  185.44121    185.44131\n    185.43535  ]\n  [ 304.96362    316.51282    331.89825   ...  184.05763    184.05759\n    184.05183  ]\n  [ 307.14413    317.82312    331.15823   ...  184.34685    184.34706\n    184.34137  ]]\n\n [[  42.33225     47.77472     49.822964  ...   24.285894    24.28604\n     24.283264 ]\n  [  42.33225     47.77472     49.822964  ...   24.285894    24.28604\n     24.283264 ]\n  [  42.33225     47.77472     49.822964  ...   24.285894    24.28604\n     24.283264 ]\n  ...\n  [ 306.40808    314.99878    322.1895    ...  180.40453    180.40443\n    180.40044  ]\n  [ 311.68524    321.31415    335.35083   ...  186.14177    186.1418\n    186.13626  ]\n  [ 314.04663    323.4371     334.69086   ...  186.45612    186.45634\n    186.45078  ]]\n\n [[  12.773008    21.607283    23.157652  ...    7.60555      7.605706\n      7.6026235]\n  [ 310.9445     316.73282    328.44702   ...  185.82776    185.82742\n    185.82193  ]\n  [ 312.62405    322.62442    328.46085   ...  187.965      187.9645\n    187.96016  ]\n  ...\n  [ 314.37003    320.5636     331.82346   ...  185.59694    185.5973\n    185.59178  ]\n  [ 307.03455    317.37097    333.14764   ...  182.90475    182.9049\n    182.89958  ]\n  [ 305.8695     315.97577    323.6126    ...  184.41917    184.41925\n    184.41406  ]]]\n","output_type":"stream"}]},{"cell_type":"code","source":"for i, dim_size in enumerate(prediction.shape):\n    print(f\"Dimension {i}: {dim_size}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:43:04.704243Z","iopub.execute_input":"2024-01-19T14:43:04.704644Z","iopub.status.idle":"2024-01-19T14:43:04.710928Z","shell.execute_reply.started":"2024-01-19T14:43:04.704608Z","shell.execute_reply":"2024-01-19T14:43:04.709757Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Dimension 0: 20\nDimension 1: 309\nDimension 2: 250880\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted_start_idxs = torch.argmax(torch.from_numpy(prediction), axis=2)\npredicted_start_idxs = predicted_start_idxs.squeeze().tolist()\nprint(predicted_start_idxs[9])","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:43:04.712275Z","iopub.execute_input":"2024-01-19T14:43:04.712711Z","iopub.status.idle":"2024-01-19T14:43:06.210770Z","shell.execute_reply.started":"2024-01-19T14:43:04.712680Z","shell.execute_reply":"2024-01-19T14:43:06.209623Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"[15, 86, 17, 88, 93, 1643, 87, 10, 5223, 34, 247696, 5223, 4081, 454, 1819, 157424, 11514, 1751, 1943, 5223, 5223, 31705, 17, 724, 19777, 17, 129699, 3266, 163213, 99047, 4081, 2694, 99047, 2373, 99047, 242708, 30689, 1325, 15, 89, 833, 37, 2167, 15, 10486, 1943, 1943, 724, 2433, 30689, 5005, 1803, 2433, 1120, 10486, 1751, 2508, 1943, 95488, 39, 902, 30689, 163213, 10486, 1943, 163213, 3266, 1943, 4593, 1943, 1094, 15, 16825, 2331, 667, 33704, 30689, 5005, 667, 5875, 724, 724, 95488, 39, 902, 30689, 724, 30689, 724, 2167, 7100, 908, 15, 2, 16502, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\nmodel_checkpoint = \"nguyenvulebinh/vi-mrc-large\"\n# model_checkpoint = \"nguyenvulebinh/vi-mrc-base\"\nnlp = pipeline('question-answering', model=model_checkpoint,\n                   tokenizer=model_checkpoint) #, device = 0)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:59:26.950314Z","iopub.execute_input":"2024-01-26T13:59:26.950689Z","iopub.status.idle":"2024-01-26T14:00:39.103492Z","shell.execute_reply.started":"2024-01-26T13:59:26.950656Z","shell.execute_reply":"2024-01-26T14:00:39.102570Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eb2e8d405e8495a899128016f8a8c6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bff3bcf1a0b499cbb8306f06db70a98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/398 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18650441e6824f31b8cf63953947b0cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"006b3faf0f464176b57c7eb00b192bc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"649483d7e48a4b468ce7e33dfe762bc4"}},"metadata":{}}]},{"cell_type":"code","source":"def calculate_accuracy(predictions, dataset):\n    total_count = len(predictions)\n    correct_count = 0\n\n    for i in range(total_count):\n        predicted_start_idxs = predictions[i]\n        example = dataset[i]\n        true_start_idx = example['start_idx']\n        true_end_idx = example['end_idx']\n\n        # Sử dụng 'any' để kiểm tra nếu ít nhất một giá trị trong danh sách predicted_start_idxs\n        # nằm trong khoảng giữa true_start_idx và true_end_idx\n        if any(true_start_idx <= predicted_start_idx <= true_end_idx for predicted_start_idx in predicted_start_idxs):\n            correct_count += 1\n\n    accuracy = (correct_count / total_count) *100 \n    accuracy = round(accuracy, 3)\n    return accuracy\n\naccuracy = calculate_accuracy(predicted_start_idxs, subset_valid_dataset)\nprint(f\"Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:45:01.914066Z","iopub.execute_input":"2024-01-19T14:45:01.914493Z","iopub.status.idle":"2024-01-19T14:45:01.937440Z","shell.execute_reply.started":"2024-01-19T14:45:01.914460Z","shell.execute_reply":"2024-01-19T14:45:01.936253Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Accuracy: 78.203\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_f1(predictions, dataset):\n    total_count = len(predictions)\n    true_positive = 0\n    false_positive = 0\n    false_negative = 0\n\n    for i in range(total_count):\n        predicted_start_idxs = predictions[i]\n        example = dataset[i]\n        true_start_idx = example['start_idx']\n        true_end_idx = example['end_idx']\n\n        # Sử dụng 'any' để kiểm tra nếu ít nhất một giá trị trong danh sách predicted_start_idxs\n        # nằm trong khoảng giữa true_start_idx và true_end_idx\n        if any(true_start_idx <= predicted_start_idx <= true_end_idx for predicted_start_idx in predicted_start_idxs):\n            if true_start_idx in predicted_start_idxs or true_end_idx in predicted_start_idxs:\n                true_positive += 1\n            else:\n                false_positive += 1\n        else:\n            false_negative += 1\n\n    # Tránh chia cho 0 khi không có true positive\n    precision = true_positive / (true_positive + false_positive) if true_positive + false_positive != 0 else 0\n    recall = true_positive / (true_positive + false_negative) if true_positive + false_negative != 0 else 0\n\n    # Tránh chia cho 0 khi cả precision và recall đều bằng 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    f1_score = f1_score * 100 \n    f1_score = round(f1_score, 3)\n    return f1_score \n    \nf1_score = calculate_f1(predicted_start_idxs, subset_valid_dataset)\nprint(f\"F1 Score: {f1_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T14:48:52.156931Z","iopub.execute_input":"2024-01-19T14:48:52.157789Z","iopub.status.idle":"2024-01-19T14:48:52.181887Z","shell.execute_reply.started":"2024-01-19T14:48:52.157752Z","shell.execute_reply":"2024-01-19T14:48:52.180788Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"F1 Score: 80.02\n","output_type":"stream"}]},{"cell_type":"code","source":"item = data['data'][2]\nitem","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:19:12.167918Z","iopub.execute_input":"2024-01-26T14:19:12.168774Z","iopub.status.idle":"2024-01-26T14:19:12.175119Z","shell.execute_reply.started":"2024-01-26T14:19:12.168743Z","shell.execute_reply":"2024-01-26T14:19:12.174095Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"{'id': 'd38ef5bf1fb82b410026ed82c8a44cae',\n 'question': 'Pháp tấn công xâm lược Việt Nam vào ngày tháng năm nào',\n 'title': 'Raymondienne',\n 'text': 'Raymondienne (hay Raymonde Dien) sinh ngày 13 tháng 5 năm 1929 tại Pháp là một người phụ nữ (đảng viên Đảng Cộng sản Pháp) đã tham gia phong trào đấu tranh chống Pháp tái xâm lược Việt Nam.',\n 'category': 'FALSE_LONG_ANSWER',\n 'is_long_answer': False}"},"metadata":{}}]},{"cell_type":"code","source":"QA_input = {\n                'question': item['question'],\n                'context': item['text']\n            }\n            \nprint(QA_input)\n            \nres = nlp(QA_input)\nres","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:19:18.884219Z","iopub.execute_input":"2024-01-26T14:19:18.884625Z","iopub.status.idle":"2024-01-26T14:19:19.336790Z","shell.execute_reply.started":"2024-01-26T14:19:18.884593Z","shell.execute_reply":"2024-01-26T14:19:19.335567Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"{'question': 'Pháp tấn công xâm lược Việt Nam vào ngày tháng năm nào', 'context': 'Raymondienne (hay Raymonde Dien) sinh ngày 13 tháng 5 năm 1929 tại Pháp là một người phụ nữ (đảng viên Đảng Cộng sản Pháp) đã tham gia phong trào đấu tranh chống Pháp tái xâm lược Việt Nam.'}\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"{'score': 3.3282427466474473e-05,\n 'start': 38,\n 'end': 62,\n 'answer': 'ngày 13 tháng 5 năm 1929'}"},"metadata":{}}]},{"cell_type":"code","source":"QA_input = {\n                'question': data['data'][1]['question'],\n                'context': data['data'][1]['text']\n            }\n            \nprint(QA_input)\n            \nres = nlp(QA_input)\nres","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:20:48.368309Z","iopub.execute_input":"2024-01-26T14:20:48.368696Z","iopub.status.idle":"2024-01-26T14:20:48.747463Z","shell.execute_reply.started":"2024-01-26T14:20:48.368667Z","shell.execute_reply":"2024-01-26T14:20:48.746375Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"{'question': 'Đất nước nào không có quân đội', 'context': 'có 23 quốc gia không có lực lượng quân đội, bao gồm: Costa Rica, Iceland, Panama, Micronesia, Quần đảo Marshall, và Vatican...'}\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"{'score': 0.4533446133136749, 'start': 53, 'end': 64, 'answer': 'Costa Rica,'}"},"metadata":{}}]},{"cell_type":"code","source":"QA_input = {\n                'question': data['data'][0]['question'],\n                'context': data['data'][0]['text']\n            }\n            \nprint(QA_input)\n            \nres = nlp(QA_input)\nres","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:21:25.640028Z","iopub.execute_input":"2024-01-26T14:21:25.640409Z","iopub.status.idle":"2024-01-26T14:21:26.148682Z","shell.execute_reply.started":"2024-01-26T14:21:25.640380Z","shell.execute_reply":"2024-01-26T14:21:26.147554Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"{'question': 'Thủ tướng Trung Quốc là gì', 'context': 'Thủ tướng Trung Quốc là nhân vật lãnh đạo chính phủ , chủ trì Quốc vụ viện gồm bốn phó thủ tướng cùng người đứng đầu các bộ và uỷ ban cấp bộ . Chủ tịch nước đương nhiệm là Tập Cận Bình , ông cũng là Tổng Bí thư của Đảng Cộng sản Trung Quốc và Chủ tịch Quân uỷ Trung Quốc , do vậy ông là lãnh đạo tối cao của Trung Quốc .'}\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"{'score': 0.9910809993743896,\n 'start': 24,\n 'end': 51,\n 'answer': 'nhân vật lãnh đạo chính phủ'}"},"metadata":{}}]},{"cell_type":"code","source":"QA_input = {\n                'question': data['data'][4]['question'],\n                'context': data['data'][4]['text']\n            }\n            \nprint(QA_input)\n            \nres = nlp(QA_input)\nres","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:21:43.316569Z","iopub.execute_input":"2024-01-26T14:21:43.316986Z","iopub.status.idle":"2024-01-26T14:21:43.679783Z","shell.execute_reply.started":"2024-01-26T14:21:43.316953Z","shell.execute_reply":"2024-01-26T14:21:43.678685Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"{'question': 'Núi nào cao nhất châu âu', 'context': 'Đỉnh núi nằm ở phần trung tâm của dãy núi Đại Kavkaz , phía đông nam của núi Elbrus , ngọn núi cao nhất của châu Âu .'}\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"{'score': 0.9292804598808289, 'start': 77, 'end': 83, 'answer': 'Elbrus'}"},"metadata":{}}]},{"cell_type":"code","source":"QA_input = {\n                'question': data['data'][6]['question'],\n                'context': data['data'][6]['text']\n            }\n            \nprint(QA_input)\n            \nres = nlp(QA_input)\nres","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:22:15.187202Z","iopub.execute_input":"2024-01-26T14:22:15.187564Z","iopub.status.idle":"2024-01-26T14:22:15.723610Z","shell.execute_reply.started":"2024-01-26T14:22:15.187536Z","shell.execute_reply":"2024-01-26T14:22:15.722494Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"{'question': 'Lê Duẩn sinh năm bao nhiêu', 'context': 'Cha ông là Lê Hiệp , làm nghề mộc . Mẹ ông là Võ Thị Đạo , làm ruộng . Sau đó ông theo gia đình về sinh sống tại làng Hậu Kiên , xã Triệu Thành cùng huyện , ở bên kia dòng sông Thạch Hãn . Khu lưu niệm Cố Tổng Bí thư Lê Duẩn hiện nay được xây dựng tại làng Hậu Kiên , xã Triệu Thành .'}\n","output_type":"stream"},{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"{'score': 3.1131599068665115e-15, 'start': 266, 'end': 267, 'answer': ','}"},"metadata":{}}]},{"cell_type":"code","source":"QA_input = {\n                'question': data['data'][7]['question'],\n                'context': data['data'][7]['text']\n            }\n            \nprint(QA_input)\n            \nres = nlp(QA_input)\nres","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:22:48.704224Z","iopub.execute_input":"2024-01-26T14:22:48.704969Z","iopub.status.idle":"2024-01-26T14:22:49.297163Z","shell.execute_reply.started":"2024-01-26T14:22:48.704933Z","shell.execute_reply":"2024-01-26T14:22:49.296041Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"{'question': 'Số lượng sọc đỏ - trắng trên quốc kỳ Mỹ tượng trưng cho điều gì', 'context': 'Phía trên bên trái của cờ Hawaii là Quốc kỳ Liên hiệp Anh. Nền lá cờ có tám sọc tượng trưng tám đảo chính của Hawaii. Từ trên xuống, các sọc có màu theo thứ tự: trắng, đỏ, xanh, trắng, đỏ, xanh, trắng, đỏ. Các đảo được tượng trưng là Hawaii, Oahu, Kauai, Kahoolawe, Lanai, Maui, Molokai, và Niihau.'}\n","output_type":"stream"},{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"{'score': 4.1773233760977746e-08,\n 'start': 92,\n 'end': 117,\n 'answer': 'tám đảo chính của Hawaii.'}"},"metadata":{}}]},{"cell_type":"code","source":"QA_input = {\n                'question': data['data'][8]['question'],\n                'context': data['data'][8]['text']\n            }\n            \nprint(QA_input)\n            \nres = nlp(QA_input)\nres","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:26:46.559108Z","iopub.execute_input":"2024-01-26T14:26:46.559952Z","iopub.status.idle":"2024-01-26T14:26:46.894387Z","shell.execute_reply.started":"2024-01-26T14:26:46.559915Z","shell.execute_reply":"2024-01-26T14:26:46.893288Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"{'question': 'Hà Nội đã trải qua bao nhiêu lần đổi tên', 'context': 'Ông sinh ngày 20 tháng 6 năm 1907 tại Hà Nội. Đây cũng là nơi ông đã trải qua thời thờ ấu.'}\n","output_type":"stream"},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"{'score': 1.1628955177284128e-12,\n 'start': 78,\n 'end': 90,\n 'answer': 'thời thờ ấu.'}"},"metadata":{}}]},{"cell_type":"code","source":"QA_input = {\n                'question': data['data'][9]['question'],\n                'context': data['data'][9]['text']\n            }\n            \nprint(QA_input)\n            \nres = nlp(QA_input)\nres","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:26:32.533511Z","iopub.execute_input":"2024-01-26T14:26:32.534721Z","iopub.status.idle":"2024-01-26T14:26:32.903203Z","shell.execute_reply.started":"2024-01-26T14:26:32.534679Z","shell.execute_reply":"2024-01-26T14:26:32.902099Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"{'question': 'Thành phố nào là thủ phủ của Ai Cập trong đế quốc La Mã', 'context': 'Lịch sử . Ai Cập bị La Mã chiếm năm 30 TCN , và Alexandria trở thành thủ phủ của tỉnh Ai Cập trong đế quốc La Mã .'}\n","output_type":"stream"},{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"{'score': 0.9997387528419495, 'start': 48, 'end': 58, 'answer': 'Alexandria'}"},"metadata":{}}]},{"cell_type":"code","source":"QA_input = {\n                'question': data['data'][10]['question'],\n                'context': data['data'][10]['text']\n            }\n            \nprint(QA_input)\n            \nres = nlp(QA_input)\nres","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:26:11.880646Z","iopub.execute_input":"2024-01-26T14:26:11.881060Z","iopub.status.idle":"2024-01-26T14:26:12.508365Z","shell.execute_reply.started":"2024-01-26T14:26:11.881025Z","shell.execute_reply":"2024-01-26T14:26:12.507332Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"{'question': 'Ngọn núi cao thứ 2 thế giới thuộc dãy núi nào', 'context': 'Dãy núi Bernina là một rặng núi thuộc rặng Trung Đông Alps của dãy núi Alps, nằm ở miền đông Thụy Sĩ và miền bắc Ý. Đây là một trong các rặng núi cao nhất của dãy núi Alps, có nhiều sông băng bao phủ. Piz Bernina (4.049 m), đỉnh cao nhất của nó, là ngọn núi cao trên 4.000 m xa nhất về phía đông trong dãy núi Alpes. Ngọn núi được nhiều người leo lên nhất là ngọn Piz Palü.'}\n","output_type":"stream"},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"{'score': 1.2064661859767511e-05, 'start': 8, 'end': 15, 'answer': 'Bernina'}"},"metadata":{}}]}]}